{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ca6dde",
   "metadata": {},
   "source": [
    "# **Creativa Data Science Bootcamp: Session 2 - Data Cleaning with Pandas**\n",
    "\n",
    "Welcome to Session 2 of the Creativa Data Science Bootcamp! In this session, we'll dive into **Pandas**, one of the most powerful tools for data manipulation in Python. Our focus will be on **Data Cleaning**, a crucial step to prepare your data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21348faa",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We begin by loading the datasets provided in CSV and Excel formats. Pandas provides the `read_csv` and `read_excel` functions for this purpose. It's essential to inspect the first few rows of your data after loading to confirm that it has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670ca9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Data:\n",
      "   Unnamed: 0     Name  Age   Salary Joining_Date   Department  Experience  \\\n",
      "0           0    Alice   25  50000.0   2023-01-01           HR         2.0   \n",
      "1           1      Bob  NaN  60000.0   2023-02-30  Engineering         3.0   \n",
      "2           2  Charlie   30  55000.0   2023-03-15  Engineering         2.0   \n",
      "3           3    Alice   25  50000.0   2023-01-01           HR         2.0   \n",
      "4           4      Eve   40      NaN   2023-04-01    Marketing         5.0   \n",
      "\n",
      "   Rating    Notes  \n",
      "0       5     Good  \n",
      "1       4  Average  \n",
      "2       4     Good  \n",
      "3       5     Good  \n",
      "4       2      NaN  \n",
      "\n",
      "Excel Data:\n",
      "   CustomerID First_Name Last_Name  Phone_Number  \\\n",
      "0        1001      Frodo   Baggins  123-545-5421   \n",
      "1        1002       Abed     Nadir  123/643/9775   \n",
      "2        1003     Walter    /White    7066950392   \n",
      "3        1004     Dwight   Schrute  123-543-2345   \n",
      "4        1005        Jon      Snow  876|678|3469   \n",
      "\n",
      "                                 Address Paying Customer Do_Not_Contact  \\\n",
      "0                  123 Shire Lane, Shire             Yes             No   \n",
      "1                    93 West Main Street              No            Yes   \n",
      "2                     298 Drugs Driveway               N            NaN   \n",
      "3  980 Paper Avenue, Pennsylvania, 18503             Yes              Y   \n",
      "4                       123 Dragons Road               Y             No   \n",
      "\n",
      "   Not_Useful_Column   age  \n",
      "0               True  28.0  \n",
      "1              False  20.0  \n",
      "2               True  28.0  \n",
      "3               True  40.0  \n",
      "4               True  49.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_data = pd.read_csv('Basics.csv')\n",
    "\n",
    "# Load the Excel file\n",
    "excel_data = pd.read_excel('Customer list.xlsx')\n",
    "\n",
    "# Display the first few rows of each to ensure they've loaded correctly\n",
    "print(\"CSV Data:\")\n",
    "print(csv_data.head())\n",
    "\n",
    "print(\"\\nExcel Data:\")\n",
    "print(excel_data.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16660191",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Once data is loaded, the next step is to understand its structure and content. We use `info()` to check the data types and identify any missing values. `describe()` provides summary statistics for numeric columns, giving insights into the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a375dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    10 non-null     int64  \n",
      " 1   Name          9 non-null      object \n",
      " 2   Age           9 non-null      object \n",
      " 3   Salary        9 non-null      float64\n",
      " 4   Joining_Date  10 non-null     object \n",
      " 5   Department    9 non-null      object \n",
      " 6   Experience    9 non-null      float64\n",
      " 7   Rating        10 non-null     int64  \n",
      " 8   Notes         9 non-null      object \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 852.0+ bytes\n",
      "None\n",
      "\n",
      "CSV Data Description:\n",
      "       Unnamed: 0        Salary  Experience     Rating\n",
      "count    10.00000      9.000000    9.000000  10.000000\n",
      "mean      4.50000  53555.555556    2.777778   3.900000\n",
      "std       3.02765   5341.140120    1.201850   0.994429\n",
      "min       0.00000  47000.000000    1.000000   2.000000\n",
      "25%       2.25000  50000.000000    2.000000   3.250000\n",
      "50%       4.50000  52000.000000    3.000000   4.000000\n",
      "75%       6.75000  60000.000000    3.000000   4.750000\n",
      "max       9.00000  60000.000000    5.000000   5.000000\n",
      "\n",
      "Excel Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CustomerID         21 non-null     int64  \n",
      " 1   First_Name         21 non-null     object \n",
      " 2   Last_Name          20 non-null     object \n",
      " 3   Phone_Number       19 non-null     object \n",
      " 4   Address            21 non-null     object \n",
      " 5   Paying Customer    21 non-null     object \n",
      " 6   Do_Not_Contact     17 non-null     object \n",
      " 7   Not_Useful_Column  21 non-null     bool   \n",
      " 8   age                20 non-null     float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(6)\n",
      "memory usage: 1.5+ KB\n",
      "None\n",
      "\n",
      "Excel Data Description:\n",
      "        CustomerID        age\n",
      "count    21.000000  20.000000\n",
      "mean   1010.952381  34.550000\n",
      "std       6.127611  13.720806\n",
      "min    1001.000000  18.000000\n",
      "25%    1006.000000  20.750000\n",
      "50%    1011.000000  32.500000\n",
      "75%    1016.000000  43.500000\n",
      "max    1020.000000  58.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explore the structure and summary of the CSV data\n",
    "print(\"CSV Data Information:\")\n",
    "print(csv_data.info())\n",
    "\n",
    "print(\"\\nCSV Data Description:\")\n",
    "print(csv_data.describe())\n",
    "\n",
    "# Repeat for the Excel data\n",
    "print(\"\\nExcel Data Information:\")\n",
    "print(excel_data.info())\n",
    "\n",
    "print(\"\\nExcel Data Description:\")\n",
    "print(excel_data.describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31e6c0",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Missing data is a common issue in real-world datasets. It's crucial to address these gaps to avoid biasing your analysis. One common approach is to impute missing values with the median of the column, which is less sensitive to outliers compared to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e4edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in CSV Data:\n",
      "Unnamed: 0      0\n",
      "Name            1\n",
      "Age             1\n",
      "Salary          1\n",
      "Joining_Date    0\n",
      "Department      1\n",
      "Experience      1\n",
      "Rating          0\n",
      "Notes           1\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Excel Data:\n",
      "CustomerID           0\n",
      "First_Name           0\n",
      "Last_Name            1\n",
      "Phone_Number         2\n",
      "Address              0\n",
      "Paying Customer      0\n",
      "Do_Not_Contact       4\n",
      "Not_Useful_Column    0\n",
      "age                  1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify missing values in the CSV data\n",
    "csv_missing = csv_data.isnull().sum()\n",
    "print(\"Missing Values in CSV Data:\")\n",
    "print(csv_missing)\n",
    "\n",
    "# Handling missing data by imputing with median for numerical columns only\n",
    "numeric_columns = csv_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "csv_data[numeric_columns] = csv_data[numeric_columns].fillna(csv_data[numeric_columns].median())\n",
    "\n",
    "# Repeat for the Excel data\n",
    "excel_missing = excel_data.isnull().sum()\n",
    "print(\"\\nMissing Values in Excel Data:\")\n",
    "print(excel_missing)\n",
    "\n",
    "numeric_columns_excel = excel_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "excel_data[numeric_columns_excel] = excel_data[numeric_columns_excel].fillna(excel_data[numeric_columns_excel].median())\n",
    "\n",
    "# Select Numeric Columns: Use select_dtypes() to select only numeric columns before applying the median.\n",
    "# Impute Median: Fill missing values only for these selected numeric columns, preventing errors from non-numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecb322",
   "metadata": {},
   "source": [
    "## Handling Duplicate Rows\n",
    "\n",
    "Duplicates can skew your data analysis, making it critical to detect and remove them. We use `duplicated()` to identify duplicates and `drop_duplicates()` to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db92dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows in CSV Data:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Name, Age, Salary, Joining_Date, Department, Experience, Rating, Notes]\n",
      "Index: []\n",
      "\n",
      "Duplicate Rows in Excel Data:\n",
      "Empty DataFrame\n",
      "Columns: [CustomerID, First_Name, Last_Name, Phone_Number, Address, Paying Customer, Do_Not_Contact, Not_Useful_Column, age]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify and remove duplicate rows in the CSV data\n",
    "csv_duplicates = csv_data[csv_data.duplicated()]\n",
    "print(\"Duplicate Rows in CSV Data:\")\n",
    "print(csv_duplicates)\n",
    "\n",
    "csv_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Repeat for the Excel data\n",
    "excel_duplicates = excel_data[excel_data.duplicated()]\n",
    "print(\"\\nDuplicate Rows in Excel Data:\")\n",
    "print(excel_duplicates)\n",
    "\n",
    "excel_data.drop_duplicates(inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee452e",
   "metadata": {},
   "source": [
    "## Ensuring Correct Data Types\n",
    "\n",
    "Data types are essential for efficient memory usage and accurate analysis. Converting columns to the correct type ensures that operations on them behave as expected. For example, ensuring an \"age\" column is numeric allows for accurate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe61aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'age' column not found in CSV data\n",
      "'Customer_ID' column not found in Excel data\n",
      "CSV Data Types:\n",
      "Unnamed: 0        int64\n",
      "Name             object\n",
      "Age              object\n",
      "Salary          float64\n",
      "Joining_Date     object\n",
      "Department       object\n",
      "Experience      float64\n",
      "Rating            int64\n",
      "Notes            object\n",
      "dtype: object\n",
      "\n",
      "Excel Data Types:\n",
      "CustomerID             int64\n",
      "First_Name            object\n",
      "Last_Name             object\n",
      "Phone_Number          object\n",
      "Address               object\n",
      "Paying Customer       object\n",
      "Do_Not_Contact        object\n",
      "Not_Useful_Column       bool\n",
      "age                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert data types if necessary, ensuring numeric data is correctly typed\n",
    "\n",
    "# For the CSV data\n",
    "if 'age' in csv_data.columns:\n",
    "    csv_data['age'] = pd.to_numeric(csv_data['age'], errors='coerce')\n",
    "else:\n",
    "    print(\"'age' column not found in CSV data\")\n",
    "\n",
    "# For the Excel data\n",
    "if 'Customer_ID' in excel_data.columns:\n",
    "    excel_data['Customer_ID'] = pd.to_numeric(excel_data['Customer_ID'], errors='coerce')\n",
    "else:\n",
    "    print(\"'Customer_ID' column not found in Excel data\")\n",
    "\n",
    "# Verify data types\n",
    "print(\"CSV Data Types:\")\n",
    "print(csv_data.dtypes)\n",
    "\n",
    "print(\"\\nExcel Data Types:\")\n",
    "print(excel_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decbf96",
   "metadata": {},
   "source": [
    "## Handling Outliers\n",
    "\n",
    "Outliers can distort analysis, especially in regression and clustering tasks. The Interquartile Range (IQR) method is a robust way to identify and optionally remove these extreme values.\n",
    "- This is just a small dataset so there is no outliers to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "944cfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers in 'age':\n",
      "Empty DataFrame\n",
      "Columns: [CustomerID, First_Name, Last_Name, Phone_Number, Address, Paying Customer, Do_Not_Contact, Not_Useful_Column, age]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# # Identify and handle outliers using the IQR method for a relevant column, e.g., 'age'\n",
    "# Q1 = excel_data['age'].quantile(0.25)\n",
    "# Q3 = excel_data['age'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# outliers = excel_data[(excel_data['age'] < (Q1 - 1.5 * IQR)) | (excel_data['age'] > (Q3 + 1.5 * IQR))]\n",
    "# print(\"Outliers in 'age':\")\n",
    "# print(outliers)\n",
    "\n",
    "# # Optionally, remove outliers\n",
    "# # excel_data = excel_data[~((excel_data['age'] < (Q1 - 1.5 * IQR)) | (excel_data['age'] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f210141",
   "metadata": {},
   "source": [
    "## Handling Inconsistent Data\n",
    "\n",
    "Data inconsistency, such as variations in categorical labels or inconsistent formats, can lead to inaccuracies. It's important to standardize these entries to ensure consistency across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f339c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 'Do_Not_Contact' values formatted 'Phone_Number':\n",
      "  Do_Not_Contact  Phone_Number\n",
      "0             No  123-545-5421\n",
      "1            Yes  123-643-9775\n",
      "2            NaN  706-695-0392\n",
      "3            Yes  123-543-2345\n",
      "4             No  876-678-3469\n"
     ]
    }
   ],
   "source": [
    "# Correcting inconsistent categorical data, e.g., standardizing 'Do_Not_Contact' column\n",
    "excel_data['Do_Not_Contact'] = excel_data['Do_Not_Contact'].replace({'Y': 'Yes', 'N': 'No'})\n",
    "\n",
    "# Correcting inconsistent phone number formatting\n",
    "excel_data['Phone_Number'] = excel_data['Phone_Number'].astype(str).replace('[^0-9]', '', regex=True)\n",
    "excel_data['Phone_Number'] = excel_data['Phone_Number'].apply(lambda x: f'{x[:3]}-{x[3:6]}-{x[6:]}' if pd.notnull(x) and x != 'nan' else '')\n",
    "\n",
    "print(\"Standardized 'Do_Not_Contact' values formatted 'Phone_Number':\")\n",
    "print(excel_data[['Do_Not_Contact', 'Phone_Number']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2781ce3",
   "metadata": {},
   "source": [
    "## Creating New Features\n",
    "\n",
    "Sometimes, new features can be created from existing ones to improve the dataset's utility for analysis. For example, splitting an address into separate columns for street, city, and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3613515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features from 'Address':\n",
      "                Street           City   State\n",
      "0       123 Shire Lane          Shire    None\n",
      "1  93 West Main Street           None    None\n",
      "2   298 Drugs Driveway           None    None\n",
      "3     980 Paper Avenue   Pennsylvania   18503\n",
      "4     123 Dragons Road           None    None\n"
     ]
    }
   ],
   "source": [
    "# Create new features, e.g., splitting 'Address' into 'Street', 'City', 'State'\n",
    "excel_data[['Street', 'City', 'State']] = excel_data['Address'].str.split(',', expand=True)\n",
    "\n",
    "print(\"New Features from 'Address':\")\n",
    "print(excel_data[['Street', 'City', 'State']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5674f83",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this session, we explored essential data cleaning techniques using Pandas, including handling missing values, duplicates, outliers, and inconsistent data. These steps are vital to preparing your data for meaningful analysis. In the next sessions, we will explore data visualization and more advanced feature engineering techniques.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
